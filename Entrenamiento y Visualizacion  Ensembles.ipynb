{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBAi6ZoWg6cI",
        "outputId": "00449014-3d73-4806-a72e-b838f661e9c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ddorado/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdanieldorado1996\u001b[0m (\u001b[33mdanieldorado1996-university-of-huelva\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Determinismo\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import gc\n",
        "from collections import Counter\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        "    classification_report\n",
        ")\n",
        "\n",
        "# Transformers\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import (\n",
        "    RobertaTokenizer,\n",
        "    AutoTokenizer,\n",
        "    RobertaTokenizerFast,\n",
        "    RobertaModel,\n",
        "    RobertaForSequenceClassification,\n",
        "    BertForSequenceClassification,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    EarlyStoppingCallback,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "\n",
        "# Datasets\n",
        "from datasets import Dataset, load_dataset, DatasetDict\n",
        "\n",
        "# Optuna\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "# Autologin a W&B\n",
        "wandb.login(key=\"59f9600e6173b66ce484fd9c8f0debf8b76f69e7\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Funciones"
      ],
      "metadata": {
        "id": "UFzVLY4vDtJy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgHVUS1gkDnx",
        "outputId": "665b9c76-e2d3-4346-cfcb-6512313e245b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\n    df_clase_0 = dft2[dft2['label'] == 0]  # Clase minoritaria\\n    df_clase_1 = dft2[dft2['label'] == 1]  # Clase mayoritaria\\n\\n# Número de ejemplos de la clase minoritaria (0)\\n    n_0 = len(df_clase_0)\\n     \\n\\n# Queremos 3 veces más ejemplos de la clase 1 que de la clase 0\\n    n_1_deseado = n_0 * 3  # Número deseado de ejemplos de clase 1\\n\\n# Hacer undersampling de la clase mayoritaria (1) para obtener la proporción 3:1\\n    df_clase_1_downsampled = df_clase_1.sample(n=n_1_deseado, replace=False, random_state=42)\\n\\n# Concatenar las dos clases después de hacer el undersampling\\n    df_balanced = pd.concat([df_clase_0, df_clase_1_downsampled])\\n\\n# Mezclar el conjunto de datos balanceado\\n    dft2_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\\n    \""
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "#Evaluar Metricas\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    preds = predictions.argmax(axis=-1)\n",
        "    probs = predictions[:, 1]  # Asumiendo binario, clase positiva = columna 1\n",
        "\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
        "    f1_macro = f1_score(labels, preds, average='macro')\n",
        "    f1_mayoritaria = f1_score(labels, preds, pos_label=1)\n",
        "    f1_minoritaria = f1_score(labels, preds, pos_label=0)\n",
        "\n",
        "    # Aquí el cambio: AUC y average precision con probs, no con preds\n",
        "    try:\n",
        "        auc = roc_auc_score(labels, probs)\n",
        "        prec_rec = average_precision_score(labels, probs)\n",
        "    except ValueError:\n",
        "        # Esto puede pasar si labels no contienen ambas clases\n",
        "        auc = float('nan')\n",
        "        prec_rec = float('nan')\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1_macro,\n",
        "        'AUC': auc,\n",
        "        'PREC_REC': prec_rec,\n",
        "        'f1_minoritaria': f1_minoritaria,\n",
        "        'f1_mayoritaria': f1_mayoritaria,\n",
        "    }\n",
        "file_path = 'trans_def.csv'\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "#limpiar Vacios\n",
        "df['about'] = df['about'].fillna('Vacia')\n",
        "df['goal'] = df['goal'].fillna('Vacia')\n",
        "df['description'] = df['description'].fillna('Vacia')\n",
        "df['motivation'] = df['motivation'].fillna('Vacia')\n",
        "\n",
        "#Crear dsataset a poartir de tabla\n",
        "def generarTextTemplate(row, opc=0):\n",
        "    text = \"\"\n",
        "\n",
        "\n",
        "    if opc == 0 or opc == 1:\n",
        "        if row['about'] is not None and pd.notna(row['about']):\n",
        "            text += row['about'] + \".\"\n",
        "            print(1)\n",
        "\n",
        "\n",
        "    if opc == 0 or opc == 2:\n",
        "        if row['description'] is not None and pd.notna(row['description']):\n",
        "            text += row['description'] + \".\"\n",
        "            print(2)\n",
        "\n",
        "    if opc == 0 or opc == 3:\n",
        "        if row['motivation'] is not None and pd.notna(row['motivation']):\n",
        "            text += row['motivation'] + \".\"\n",
        "            print(3)\n",
        "\n",
        "    if opc == 0 or opc == 4:\n",
        "        if row['goal'] is not None and pd.notna(row['goal']):\n",
        "            text += str(row['goal']) + \".\"\n",
        "            print(4)\n",
        "\n",
        "    return pd.Series({'text': text, 'label': 1 if row['status'] == 'SI' else 0})\n",
        "\n",
        "\n",
        "#adapta lo necesario para generar  los conjuntos traint test y valid\n",
        "def preparar_datasets(df, dft,w ,st=3, model_name='PlanTL-GOB-ES/longformer-base-4096-bne-es'):\n",
        "\n",
        "    df2 = pd.DataFrame([generarTextTemplate(df.iloc[i],w) for i in range(len(df))])\n",
        "    dft2 = pd.DataFrame([generarTextTemplate(dft.iloc[i],w) for i in range(len(dft))])\n",
        "\n",
        "# Separar el dataframe por clase\n",
        "    df_clase_0 = df2[df2['label'] == 0]  # Clase minoritaria\n",
        "    df_clase_1 = df2[df2['label'] == 1]  # Clase mayoritaria\n",
        "\n",
        "# Número de ejemplos de la clase minoritaria (0)\n",
        "    n_0 = len(df_clase_0)\n",
        "\n",
        "\n",
        "\n",
        "    n_1_deseado = n_0 * st  # Número deseado de ejemplos de clase 1\n",
        "\n",
        "# Hacer undersampling de la clase mayoritaria\n",
        "    df_clase_1_downsampled = df_clase_1.sample(n=n_1_deseado, replace=False, random_state=42)\n",
        "\n",
        "# Concatenar las dos clases después de hacer el undersampling\n",
        "    df_balanced = pd.concat([df_clase_0, df_clase_1_downsampled])\n",
        "\n",
        "# Mezclar el conjunto de datos balanceado\n",
        "    df2_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
        "    dataset = Dataset.from_pandas(df2_balanced)\n",
        "    datasett = Dataset.from_pandas(dft2)\n",
        "# Verifica el límite del modelo cargado\n",
        "    model_max_length = tokenizer.model_max_length\n",
        "\n",
        "    def tokenize_function(example):\n",
        "        return tokenizer(\n",
        "        example[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=min(model_max_length, 512),\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Aplicar tokenización\n",
        "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "    tokenized_datasett = datasett.map(tokenize_function, batched=True)\n",
        "\n",
        "    # Convertir a tensores\n",
        "    tokenized_dataset = tokenized_dataset.with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "    tokenized_datasett = tokenized_datasett.with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "    # Dividir datasets\n",
        "    split_dataset = tokenized_dataset.train_test_split(test_size=0.3, seed=42)\n",
        "    train_data = split_dataset['train']\n",
        "    valid_data = split_dataset['test']\n",
        "    test_data = tokenized_datasett\n",
        "\n",
        "    return train_data, valid_data, test_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#optuna\n",
        "def objective(trial):\n",
        "    global modelN\n",
        "    global bauc\n",
        "    global v\n",
        "    global path\n",
        "\n",
        "    # Limpiar la memoria de la GPU antes de cada nueva iteración\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # Selección de parámetros para la optimización\n",
        "    learning_rate = trial.suggest_categorical(\"learning_rate\",  [5e-6,1e-5, 2e-5, 3e-5, 5e-5,1e-4])\n",
        "    weight_decay = trial.suggest_categorical(\"weight_decay\", [0.1, 0.01, 0.001])\n",
        "    optimizer = trial.suggest_categorical(\"optimizer\", [ \"adamw_torch\", \"adafactor\", \"schedule_free_radam\"])\n",
        "    bs = trial.suggest_categorical(\"batch_size\", [16,32])\n",
        "\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(modelN, do_lower_case=True)\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        modelN, num_labels=2, force_download=True)\n",
        "\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        save_strategy=\"no\",\n",
        "        logging_strategy=\"no\",\n",
        "        eval_strategy=\"epoch\",\n",
        "\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=bs,\n",
        "        per_device_eval_batch_size=bs,\n",
        "        num_train_epochs=10,\n",
        "        weight_decay=weight_decay,\n",
        "\n",
        "        metric_for_best_model='AUC',\n",
        "\n",
        "        optim=optimizer,\n",
        "        fp16=False,\n",
        "        dataloader_num_workers=0,\n",
        "\n",
        "    )\n",
        "\n",
        "    # Inicializar el trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_data,\n",
        "        eval_dataset=valid_data,\n",
        "        tokenizer=tokenizer,\n",
        "         callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] ,\n",
        "        compute_metrics=compute_metrics  # Incluir cálculo de F1\n",
        "    )\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    trainer.train()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Obtener las predicciones y calcular el F1 score\n",
        "        predictions, labels, _ = trainer.predict(test_data)\n",
        "        preds = predictions.argmax(axis=-1)\n",
        "\n",
        "        probs = predictions[:, 1]\n",
        "\n",
        "        df = pd.DataFrame({\n",
        "        'label': labels,\n",
        "        'predicted_label': preds,\n",
        "        'probability_class_1': probs\n",
        "      })\n",
        "\n",
        "    # Guardar como CSV\n",
        "        df.to_csv(f\"{path}_preds\", index=False)\n",
        "        preds = predictions.argmax(axis=-1)\n",
        "        probs = predictions[:, 1]\n",
        "\n",
        "        accuracy = accuracy_score(labels, preds)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
        "        f1_macro = f1_score(labels, preds, average='macro')\n",
        "        f1_mayoritaria = f1_score(labels, preds, pos_label=1)\n",
        "        f1_minoritaria = f1_score(labels, preds, pos_label=0)\n",
        "\n",
        "\n",
        "\n",
        "        auc = roc_auc_score(labels, probs)\n",
        "        prec_rec = average_precision_score(labels, probs)\n",
        "\n",
        "\n",
        "    trial.set_user_attr(\"accuracy\", accuracy)\n",
        "    trial.set_user_attr(\"precision\", precision)\n",
        "    trial.set_user_attr(\"recall\", recall)\n",
        "    trial.set_user_attr(\"f1-macro\", f1_macro)\n",
        "    trial.set_user_attr(\"f1_minoritaria\", f1_minoritaria)\n",
        "    trial.set_user_attr(\"f1_mayoritaria\", f1_mayoritaria)\n",
        "    trial.set_user_attr(\"AUC\", auc)\n",
        "    trial.set_user_attr(\"PREC_REC\", prec_rec)\n",
        "    if auc>bauc:\n",
        "      bauc=auc\n",
        "\n",
        "      print(auc,\" se  guardo   \\n\")\n",
        "\n",
        "      model_path = path\n",
        "\n",
        "\n",
        "      trainer.save_model(model_path)\n",
        "\n",
        "      trainer.tokenizer.save_pretrained(model_path)\n",
        "\n",
        "\n",
        "    else:\n",
        "            print(auc,\" no se  guardo   \\n\")\n",
        "    # Liberar memoria de los objetos trainer y modelo\n",
        "    del model\n",
        "    del trainer\n",
        "\n",
        "\n",
        "    return auc\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Busqueda Proporcion"
      ],
      "metadata": {
        "id": "Bpb7FOpsDw2a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1_hYxT1mL3-",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#dividir train test\n",
        "dftr, dft = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    stratify=df['status'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "da6fe7eab97247599cede334a9db8514",
            "db60ede10cc24c83bb7730dc9117f6bb",
            "7d65ebf201f54656b744af54b3197607",
            "eabf745916bc4b0f97c81ca5bf40e868",
            "cc731b2fc273436f809fa9da8ce3c4b1",
            "ab9e9b02a7514030aed3bc7ea8f2e62a"
          ]
        },
        "id": "W6yJnkgw-Cit",
        "outputId": "91147976-ca3c-4bef-e6e1-369d2ec3271b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da6fe7eab97247599cede334a9db8514",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/592 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db60ede10cc24c83bb7730dc9117f6bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/471 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dispositivo disponible: 0 (NVIDIA GeForce RTX 4090)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d65ebf201f54656b744af54b3197607",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eabf745916bc4b0f97c81ca5bf40e868",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc731b2fc273436f809fa9da8ce3c4b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at PlanTL-GOB-ES/roberta-base-bne and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipykernel_67582/2082463639.py:44: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "2025-05-23 09:24:43.287680: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-23 09:24:43.290518: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-05-23 09:24:43.296010: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747985083.305598   67582 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747985083.308752   67582 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1747985083.316399   67582 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1747985083.316409   67582 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1747985083.316410   67582 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1747985083.316411   67582 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-05-23 09:24:43.319607: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab9e9b02a7514030aed3bc7ea8f2e62a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/ddorado/Entrenamiento/wandb/run-20250523_092443-pxr6g8cl</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/danieldorado1996-university-of-huelva/huggingface/runs/pxr6g8cl' target=\"_blank\">trainer_output</a></strong> to <a href='https://wandb.ai/danieldorado1996-university-of-huelva/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/danieldorado1996-university-of-huelva/huggingface' target=\"_blank\">https://wandb.ai/danieldorado1996-university-of-huelva/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/danieldorado1996-university-of-huelva/huggingface/runs/pxr6g8cl' target=\"_blank\">https://wandb.ai/danieldorado1996-university-of-huelva/huggingface/runs/pxr6g8cl</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='36' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 36/130 00:11 < 00:31, 3.00 it/s, Epoch 2.69/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Auc</th>\n",
              "      <th>Prec Rec</th>\n",
              "      <th>F1 Minoritaria</th>\n",
              "      <th>F1 Mayoritaria</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.695144</td>\n",
              "      <td>0.483146</td>\n",
              "      <td>0.241573</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.325758</td>\n",
              "      <td>0.575708</td>\n",
              "      <td>0.603218</td>\n",
              "      <td>0.651515</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.694927</td>\n",
              "      <td>0.483146</td>\n",
              "      <td>0.241573</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.325758</td>\n",
              "      <td>0.581775</td>\n",
              "      <td>0.609535</td>\n",
              "      <td>0.651515</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ddorado/anaconda3/envs/cudaenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/home/ddorado/anaconda3/envs/cudaenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 55\u001b[0m\n\u001b[1;32m     44\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     45\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     46\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics  \u001b[38;5;66;03m# Incluir cálculo de F1\u001b[39;00m\n\u001b[1;32m     52\u001b[0m )\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Entrenar el modelo\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Usar no_grad para evitar acumular gradientes innecesarios en la validación\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     59\u001b[0m        \n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Obtener las predicciones y calcular el F1 score\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/cudaenv/lib/python3.11/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2246\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   2247\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   2248\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   2249\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   2250\u001b[0m     )\n",
            "File \u001b[0;32m~/anaconda3/envs/cudaenv/lib/python3.11/site-packages/transformers/trainer.py:2560\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2554\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2556\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2557\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2558\u001b[0m )\n\u001b[1;32m   2559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2560\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2563\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2564\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2565\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2566\u001b[0m ):\n\u001b[1;32m   2567\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2568\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[0;32m~/anaconda3/envs/cudaenv/lib/python3.11/site-packages/transformers/trainer.py:3782\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   3780\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 3782\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3784\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
            "File \u001b[0;32m~/anaconda3/envs/cudaenv/lib/python3.11/site-packages/accelerate/accelerator.py:2473\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2471\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2473\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/envs/cudaenv/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    650\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/envs/cudaenv/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m _engine_run_backward(\n\u001b[1;32m    354\u001b[0m     tensors,\n\u001b[1;32m    355\u001b[0m     grad_tensors_,\n\u001b[1;32m    356\u001b[0m     retain_graph,\n\u001b[1;32m    357\u001b[0m     create_graph,\n\u001b[1;32m    358\u001b[0m     inputs,\n\u001b[1;32m    359\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    360\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    361\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/envs/cudaenv/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "for il in range(1,6):\n",
        "    for v in range(1,5):\n",
        "        ruta=None\n",
        "        train_data, valid_data, test_data=preparar_datasets(dftr, dft,v,st=il, model_name= \"'dccuchile/bert-base-spanish-wwm-uncased'\")\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "        print(f\"Dispositivo disponible: {torch.cuda.current_device()} ({torch.cuda.get_device_name()})\")\n",
        "\n",
        "        learning_rate = 1e-5\n",
        "        weight_decay = 0.01\n",
        "        optimizer = \"adafactor\"\n",
        "        bs = 16\n",
        "\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained( \"'dccuchile/bert-base-spanish-wwm-uncased'\", do_lower_case=True)\n",
        "\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "         \"'dccuchile/bert-base-spanish-wwm-uncased'\", num_labels=2,force_download=True)\n",
        "\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "\n",
        "        eval_strategy=\"epoch\",\n",
        "\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=bs,\n",
        "        per_device_eval_batch_size=bs,\n",
        "        num_train_epochs=10,\n",
        "        weight_decay=weight_decay,\n",
        "        save_strategy=\"no\",\n",
        "        logging_strategy=\"no\",\n",
        "        metric_for_best_model='AUC',\n",
        "\n",
        "        optim=optimizer,\n",
        "        fp16=False,\n",
        "        dataloader_num_workers=0,\n",
        "\n",
        "    )\n",
        "\n",
        "    # Inicializar el trainer\n",
        "        trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_data,\n",
        "        eval_dataset=valid_data,\n",
        "        tokenizer=tokenizer,\n",
        "         callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] ,\n",
        "        compute_metrics=compute_metrics  # Incluir cálculo de F1\n",
        "    )\n",
        "\n",
        "    # Entrenar el modelo\n",
        "        trainer.train()\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "\n",
        "            predictions, labels, _ = trainer.predict(test_data)\n",
        "            preds = predictions.argmax(axis=-1)\n",
        "\n",
        "        probs = predictions[:, 1]\n",
        "\n",
        "\n",
        "        df = pd.DataFrame({\n",
        "        'label': labels,\n",
        "        'predicted_label': preds,\n",
        "        'probability_class_1': probs\n",
        "      })\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        preds = predictions.argmax(axis=-1)\n",
        "        probs = predictions[:, 1]\n",
        "\n",
        "        accuracy = accuracy_score(labels, preds)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
        "        f1_macro = f1_score(labels, preds, average='macro')\n",
        "        f1_mayoritaria = f1_score(labels, preds, pos_label=1)\n",
        "        f1_minoritaria = f1_score(labels, preds, pos_label=0)\n",
        "\n",
        "\n",
        "        auc = roc_auc_score(labels, probs)\n",
        "        prec_rec = average_precision_score(labels, probs)\n",
        "        print(f\"Campo {v} Tamano {il}    :  {auc}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        model_path = f\"./BtTam/Bt{il}-{v}\"\n",
        "\n",
        "        del model\n",
        "        del trainer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Busqueda Optuna"
      ],
      "metadata": {
        "id": "VoOoUecHDnOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dftr, dft = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    stratify=df['status'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pw76TbZrAmC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Busqueda de Hiperparametros\n",
        "modelN='dccuchile/bert-base-spanish-wwm-uncased'\n",
        "for v in range(1,5):\n",
        "\n",
        "    path = f'./Bt/Bt{v}.csv'\n",
        "\n",
        "    train_data, valid_data, test_data = preparar_datasets(\n",
        "        dftr, dft, v, 2,model_name=modelN\n",
        "    )\n",
        "\n",
        "    bauc = 0\n",
        "\n",
        "\n",
        "    # Limpieza de GPU\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "    print(f\"Dispositivo disponible: {torch.cuda.current_device()} ({torch.cuda.get_device_name()})\")\n",
        "\n",
        "    # Crear el estudio de Optuna\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=15)\n",
        "\n",
        "    df = study.trials_dataframe()\n",
        "\n",
        "\n",
        "    csv_filename = f\"./Bt/optuna_Bt{v}.csv\"\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "\n",
        "\n",
        "    best_trial = study.best_trial\n",
        "    best_params = best_trial.params\n",
        "\n",
        "    best_params[\"model_name\"] = f'Bt{v}'\n",
        "    best_params[\"objective_value\"] = best_trial.value\n",
        "\n",
        "    # CSV acumulativo\n",
        "    csv_accum_path = \"./best_trials_acumulado.csv\"\n",
        "\n",
        "    df_best = pd.DataFrame([best_params])\n",
        "\n",
        "    if os.path.exists(csv_accum_path):\n",
        "        df_existing = pd.read_csv(csv_accum_path)\n",
        "        df_combined = pd.concat([df_existing, df_best], ignore_index=True, sort=True)\n",
        "    else:\n",
        "        df_combined = df_best\n",
        "\n",
        "    df_combined.to_csv(csv_accum_path, index=False)\n",
        "\n",
        "    print(f\"Experimento {v} completado. Mejor trial guardado en {csv_accum_path}\")\n",
        "\n",
        "modelN=\"PlanTL-GOB-ES/roberta-base-bne\"\n",
        "for v in range(1,5):\n",
        "\n",
        "    path = f'./Rb/Rb{v}.csv'\n",
        "\n",
        "    train_data, valid_data, test_data = preparar_datasets(\n",
        "        dftr, dft, v, 2,model_name=modelN\n",
        "    )\n",
        "\n",
        "    bauc = 0\n",
        "\n",
        "\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "    print(f\"Dispositivo disponible: {torch.cuda.current_device()} ({torch.cuda.get_device_name()})\")\n",
        "\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=15)\n",
        "\n",
        "    df = study.trials_dataframe()\n",
        "\n",
        "    csv_filename = f\"./Rb/optuna_Rb{v}.csv\"\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "\n",
        "    best_trial = study.best_trial\n",
        "    best_params = best_trial.params\n",
        "\n",
        "\n",
        "    best_params[\"model_name\"] = f'Rb{v}'\n",
        "    best_params[\"objective_value\"] = best_trial.value\n",
        "\n",
        "    # CSV acumulativo\n",
        "    csv_accum_path = \"./best_trials_acumulado.csv\"\n",
        "\n",
        "    df_best = pd.DataFrame([best_params])\n",
        "\n",
        "    if os.path.exists(csv_accum_path):\n",
        "        df_existing = pd.read_csv(csv_accum_path)\n",
        "        df_combined = pd.concat([df_existing, df_best], ignore_index=True, sort=True)\n",
        "    else:\n",
        "        df_combined = df_best\n",
        "\n",
        "    df_combined.to_csv(csv_accum_path, index=False)\n",
        "\n",
        "    print(f\"Experimento {v} completado. Mejor trial guardado en {csv_accum_path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "1JLRcKJqDZpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Entrenamiento Mejores Hiperparametros"
      ],
      "metadata": {
        "id": "klRqXS_rD2dM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "df = pd.read_csv(\"best_trials_acumulado.csv\")\n",
        "df.columns = ['batch_size', 'learning_rate', 'experiment_id', 'score', 'optimizer', 'weight_decay']\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "for idx, row in df.iterrows():\n",
        "    batch_size = int(row['batch_size'])\n",
        "    learning_rate = float(row['learning_rate'])\n",
        "    optimizer_name = row['optimizer']\n",
        "    weight_decay = float(row['weight_decay'])\n",
        "    experiment_id = row['experiment_id']\n",
        "    prefix = experiment_id[:2]\n",
        "    num_id = int(experiment_id[2:])\n",
        "    if experiment_id.startswith(\"Bt\"):\n",
        "        model_name = \"dccuchile/bert-base-spanish-wwm-uncased\"\n",
        "\n",
        "    elif experiment_id.startswith(\"Rb\"):\n",
        "        model_name = \"PlanTL-GOB-ES/roberta-base-bne\"\n",
        "\n",
        "\n",
        "    for i in range(0,3):\n",
        "\n",
        "        train_data, valid_data, test_data=preparar_datasets(bootstraps[i], dft,w=num_id, model_name= model_name)\n",
        "\n",
        "        print(f\"Entrenando experimento {idx} con batch_size={batch_size}, lr={learning_rate}, optimizer={optimizer_name}, weight_decay={weight_decay}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained( model_name, do_lower_case=True)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "    # Configuración de entrenamiento\n",
        "        training_args = TrainingArguments(\n",
        "\n",
        "        eval_strategy=\"epoch\",\n",
        "\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=batch_size,            # Tamaño del lote de entrenamiento\n",
        "        per_device_eval_batch_size=batch_size,             # Tamaño del lote de evaluación\n",
        "        num_train_epochs=10,\n",
        "        weight_decay=weight_decay,\n",
        "        save_strategy=\"no\",\n",
        "        logging_strategy=\"no\",\n",
        "        metric_for_best_model='AUC',\n",
        "\n",
        "        optim=optimizer_name,\n",
        "        fp16=False,\n",
        "        dataloader_num_workers=0,\n",
        "\n",
        "    )\n",
        "\n",
        "        trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_data,\n",
        "        eval_dataset=valid_data,\n",
        "        tokenizer=tokenizer,\n",
        "         callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] ,\n",
        "        compute_metrics=compute_metrics  # Incluir cálculo de F1\n",
        "    )\n",
        "\n",
        "        trainer.train()\n",
        "        with torch.no_grad():\n",
        "\n",
        "        # Obtener las predicciones y calcular el F1 score\n",
        "            predictions, labels, _ = trainer.predict(test_data)\n",
        "            preds = predictions.argmax(axis=-1)\n",
        "\n",
        "        probs = predictions[:, 1]  # Asumiendo binario, clase positiva = columna 1\n",
        "\n",
        "     # Guardar etiquetas y predicciones en un DataFrame\n",
        "        df = pd.DataFrame({\n",
        "        'label': labels,\n",
        "        'predicted_label': preds,\n",
        "        'probability_class_1': probs\n",
        "      })\n",
        "\n",
        "    # Guardar como CSV\n",
        "        df.to_csv(f\"./{prefix}/{prefix}B{i}-M{num_id}_preds\", index=False)\n",
        "\n",
        "\n",
        "        preds = predictions.argmax(axis=-1)\n",
        "        probs = predictions[:, 1]  # Asumiendo binario, clase positiva = columna 1\n",
        "\n",
        "        accuracy = accuracy_score(labels, preds)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
        "        f1_macro = f1_score(labels, preds, average='macro')\n",
        "        f1_mayoritaria = f1_score(labels, preds, pos_label=1)\n",
        "        f1_minoritaria = f1_score(labels, preds, pos_label=0)\n",
        "\n",
        "    # Aquí el cambio: AUC y average precision con probs, no con preds\n",
        "\n",
        "        auc = roc_auc_score(labels, probs)\n",
        "        prec_rec = average_precision_score(labels, probs)\n",
        "        print(f'{accuracy} {f1_macro} {auc}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        model_path = f\"./{prefix}/{prefix}B{i}-M{num_id}\"  # Choose a path in your Google Drive\n",
        "\n",
        "\n",
        "        trainer.save_model(model_path)\n",
        "\n",
        "        trainer.tokenizer.save_pretrained(model_path)\n",
        "\n",
        "\n",
        "\n",
        "    # Liberar memoria de los objetos trainer y modelo\n",
        "        del model\n",
        "        del trainer\n",
        "\n"
      ],
      "metadata": {
        "id": "lcGnTI7oD16F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluar Resultados"
      ],
      "metadata": {
        "id": "WZWNNquzEJRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, roc_auc_score, average_precision_score\n",
        ")\n",
        "from scipy.special import expit\n",
        "\n",
        "def evaluate_ensemble(files):\n",
        "    logits_list = []\n",
        "    labels = None\n",
        "\n",
        "    for file in files:\n",
        "        df = pd.read_csv(file )\n",
        "        if labels is None:\n",
        "            labels = df['label'].values\n",
        "        else:\n",
        "            assert np.array_equal(labels, df['label'].values), f\"Labels mismatch in {file}\"\n",
        "        logits = df['probability_class_1'].values\n",
        "        logits_list.append(logits)\n",
        "\n",
        "    logits_array = np.array(logits_list)\n",
        "    probs_array = expit(logits_array)\n",
        "    ensemble_probs = np.mean(probs_array, axis=0)\n",
        "    ensemble_preds = (ensemble_probs >= 0.5).astype(int)\n",
        "\n",
        "    accuracy = accuracy_score(labels, ensemble_preds)\n",
        "    precision = precision_score(labels, ensemble_preds, average='macro')\n",
        "    recall = recall_score(labels, ensemble_preds, average='macro')\n",
        "    f1_macro = f1_score(labels, ensemble_preds, average='macro')\n",
        "    f1_pos = f1_score(labels, ensemble_preds, pos_label=1)\n",
        "    f1_neg = f1_score(labels, ensemble_preds, pos_label=0)\n",
        "    auc = roc_auc_score(labels, ensemble_probs)\n",
        "    avg_prec = average_precision_score(labels, ensemble_probs)\n",
        "\n",
        "    print(\"\\n📊 Métricas del ensemble:\")\n",
        "    print(f\"Accuracy:               {accuracy:.4f}\")\n",
        "    print(f\"Precision (macro):      {precision:.4f}\")\n",
        "    print(f\"Recall (macro):         {recall:.4f}\")\n",
        "    print(f\"F1 Score (macro):       {f1_macro:.4f}\")\n",
        "    print(f\"F1 clase positiva (1):  {f1_pos:.4f}\")\n",
        "    print(f\"F1 clase negativa (0):  {f1_neg:.4f}\")\n",
        "    print(f\"ROC AUC:                {auc:.4f}\")\n",
        "    print(f\"Average Precision:      {avg_prec:.4f}\")\n",
        "\n",
        "    return ensemble_probs, labels\n",
        "\n",
        "# Grupos de archivos (sin extensión)\n",
        "file_groups = [\n",
        "[\"./Rb/RbB0-M1_preds\", \"./Rb/RbB1-M1_preds\", \"./Rb/RbB2-M1_preds\"],\n",
        "[\"./Rb/RbB0-M2_preds\", \"./Rb/RbB1-M2_preds\", \"./Rb/RbB2-M2_preds\"],\n",
        "[\"./Rb/RbB0-M3_preds\", \"./Rb/RbB1-M3_preds\", \"./Rb/RbB2-M3_preds\"],\n",
        "[\"./Rb/RbB0-M4_preds\", \"./Rb/RbB1-M4_preds\", \"./Rb/RbB2-M4_preds\"]\n",
        "]\n",
        "\n",
        "file_groups =   [ [\"./Bt/BtB0-M1_preds\", \"./Bt/BtB1-M1_preds\", \"./Bt/BtB2-M1_preds\"],\n",
        "    [\"./Bt/BtB0-M2_preds\", \"./Bt/BtB1-M2_preds\", \"./Bt/BtB2-M2_preds\"],\n",
        "    [\"./Bt/BtB0-M3_preds\", \"./Bt/BtB1-M3_preds\", \"./Bt/BtB2-M3_preds\"],\n",
        "    [\"./Bt/BtB0-M4_preds\", \"./Bt/BtB1-M4_preds\", \"./Bt/BtB2-M4_preds\"]\n",
        "]\n",
        "all_ensemble_probs = []\n",
        "all_labels = None\n",
        "\n",
        "for idx, group in enumerate(file_groups, 1):\n",
        "    print(f\"\\n==== Evaluando grupo {idx} ====\")\n",
        "    probs, labels = evaluate_ensemble(group)\n",
        "    all_ensemble_probs.append(probs)\n",
        "    if all_labels is None:\n",
        "        all_labels = labels\n",
        "    else:\n",
        "        assert np.array_equal(all_labels, labels), \"Labels mismatch between groups\"\n",
        "\n",
        "# Ensemble general con las probabilidades promedio de los 4 grupos\n",
        "all_ensemble_probs_array = np.array(all_ensemble_probs)\n",
        "final_ensemble_probs = np.mean(all_ensemble_probs_array, axis=0)\n",
        "final_ensemble_preds = (final_ensemble_probs >= 0.5).astype(int)\n",
        "\n",
        "accuracy = accuracy_score(all_labels, final_ensemble_preds)\n",
        "precision = precision_score(all_labels, final_ensemble_preds, average='macro')\n",
        "recall = recall_score(all_labels, final_ensemble_preds, average='macro')\n",
        "f1_macro = f1_score(all_labels, final_ensemble_preds, average='macro')\n",
        "f1_pos = f1_score(all_labels, final_ensemble_preds, pos_label=1)\n",
        "f1_neg = f1_score(all_labels, final_ensemble_preds, pos_label=0)\n",
        "auc = roc_auc_score(all_labels, final_ensemble_probs)\n",
        "avg_prec = average_precision_score(all_labels, final_ensemble_probs)\n",
        "\n",
        "print(\"\\n==== Métricas del ENSEMBLE GENERAL (todos los boostraps) ====\")\n",
        "print(f\"Accuracy:               {accuracy:.4f}\")\n",
        "print(f\"Precision (macro):      {precision:.4f}\")\n",
        "print(f\"Recall (macro):         {recall:.4f}\")\n",
        "print(f\"F1 Score (macro):       {f1_macro:.4f}\")\n",
        "print(f\"F1 clase positiva (1):  {f1_pos:.4f}\")\n",
        "print(f\"F1 clase negativa (0):  {f1_neg:.4f}\")\n",
        "print(f\"ROC AUC:                {auc:.4f}\")\n",
        "print(f\"Average Precision:      {avg_prec:.4f}\")"
      ],
      "metadata": {
        "id": "R1ZdXmGHEHEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    average_precision_score\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "def evaluate_ensemble(files):\n",
        "    probabilities = []\n",
        "    predictions = []\n",
        "    labels = None\n",
        "\n",
        "    for file in files:\n",
        "        df = pd.read_csv(file)\n",
        "        probabilities.append(df['probability_class_1'].values)\n",
        "        predictions.append(df['predicted_label'].values)\n",
        "        if labels is None:\n",
        "            labels = df['label'].values\n",
        "\n",
        "    probs_array = np.array(probabilities)\n",
        "    preds_array = np.array(predictions)\n",
        "    ensemble_probs = np.mean(probs_array, axis=0)\n",
        "    ensemble_preds = (ensemble_probs >= 0.5).astype(int)\n",
        "\n",
        "    accuracy = accuracy_score(labels, ensemble_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, ensemble_preds, average='macro')\n",
        "    auc = roc_auc_score(labels, ensemble_probs)\n",
        "    ap = average_precision_score(labels, ensemble_probs)\n",
        "    f1_minoritaria = f1_score(labels, ensemble_preds, pos_label=0)\n",
        "    print(\"\\n📊 Métricas del ensemble:\")\n",
        "    print(f\"Accuracy:           {accuracy:.4f}\")\n",
        "    print(f\"Precision:          {precision:.4f}\")\n",
        "    print(f\"Recall:             {recall:.4f}\")\n",
        "    print(f\"F1 Minoritaria:     {f1_minoritaria:.4f}\")\n",
        "    print(f\"F1 Score (Macro):   {f1:.4f}\")\n",
        "    print(f\"ROC AUC:            {auc:.4f}\")\n",
        "    print(f\"Average Precision:  {ap:.4f}\")\n",
        "\n",
        "    return ensemble_preds, ensemble_probs, labels, auc\n",
        "\n",
        "def majority_vote_with_auc(ensemble_preds_list, ensemble_probs_list, auc_scores, labels):\n",
        "    preds_array = np.array(ensemble_preds_list)\n",
        "    n_models, n_samples = preds_array.shape\n",
        "\n",
        "    final_preds = []\n",
        "    for i in range(n_samples):\n",
        "        votes = preds_array[:, i]\n",
        "        vote_count = np.bincount(votes)\n",
        "        if len(vote_count) == 1 or vote_count[0] != vote_count[1]:\n",
        "            final_preds.append(np.argmax(vote_count))\n",
        "        else:\n",
        "            best_model_idx = np.argmax(auc_scores)\n",
        "            final_preds.append(ensemble_preds_list[best_model_idx][i])\n",
        "\n",
        "    final_preds = np.array(final_preds)\n",
        "    probs_array = np.array(ensemble_probs_list)\n",
        "    final_probs = np.mean(probs_array, axis=0)\n",
        "\n",
        "    accuracy = accuracy_score(labels, final_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, final_preds, average='macro')\n",
        "    f1_mayoritaria = f1_score(labels, final_preds, pos_label=1)\n",
        "    f1_minoritaria = f1_score(labels, final_preds, pos_label=0)\n",
        "    auc = roc_auc_score(labels, final_probs)\n",
        "    ap = average_precision_score(labels, final_probs)\n",
        "\n",
        "    print(\"\\n🔚 Ensemble final (votación + AUC en empates):\")\n",
        "    print(f\"Accuracy:           {accuracy:.4f}\")\n",
        "    print(f\"Precision:          {precision:.4f}\")\n",
        "    print(f\"Recall:             {recall:.4f}\")\n",
        "\n",
        "    print(f\"F1 Macro:           {f1:.4f}\")\n",
        "    print(f\"F1 Mayoritaria:     {f1_mayoritaria:.4f}\")\n",
        "    print(f\"F1 Minoritaria:     {f1_minoritaria:.4f}\")\n",
        "    print(f\"ROC AUC:            {auc:.4f}\")\n",
        "    print(f\"Average Precision:  {ap:.4f}\")\n",
        "     # Guardar resultados en CSV\n",
        "    results_df = pd.DataFrame({\n",
        "        'label': labels,\n",
        "        'predicted_label': final_preds,\n",
        "        'probability_class_1': final_probs\n",
        "    })\n",
        "\n",
        "    results_df.to_csv(\"ensemble_final_predictions.csv\", index=False)\n",
        "    print(\"\\n📁 Resultados guardados en 'ensemble_final_predictions.csv'\")\n",
        "    fpr, tpr, _ = roc_curve(labels, final_probs)\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Curva ROC - Ensemble Final RoBERTa')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True)\n",
        "    plt.savefig(\"ensemble_final_roc_curve.png\")\n",
        "    plt.close()\n",
        "    print(\"🖼️ Curva ROC guardada como 'ensemble_final_roc_curve.png'\")\n",
        "\n",
        "    # 📊 Guardar matriz de confusión\n",
        "    cm = confusion_matrix(labels, final_preds)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title('Matriz de Confusión - Ensemble Final RoBERTa')\n",
        "    plt.savefig(\"ensemble_final_confusion_matrix.png\")\n",
        "    plt.close()\n",
        "    print(\"🖼️ Matriz de confusión guardada como 'ensemble_final_confusion_matrix.png'\")\n",
        "\n",
        "\n",
        "# Archivos de predicciones para Bt\n",
        "file_groups =   [ [\"./Bt/BtB0-M1_preds\", \"./Bt/BtB1-M1_preds\", \"./Bt/BtB2-M1_preds\"],\n",
        "    [\"./Bt/BtB0-M2_preds\", \"./Bt/BtB1-M2_preds\", \"./Bt/BtB2-M2_preds\"],\n",
        "    [\"./Bt/BtB0-M3_preds\", \"./Bt/BtB1-M3_preds\", \"./Bt/BtB2-M3_preds\"],\n",
        "    [\"./Bt/BtB0-M4_preds\", \"./Bt/BtB1-M4_preds\", \"./Bt/BtB2-M4_preds\"]]\n",
        "\n",
        "\n",
        "ensemble_preds_list = []\n",
        "ensemble_probs_list = []\n",
        "auc_scores = []\n",
        "labels = None\n",
        "\n",
        "# Evaluar cada ensemble individual (por modelo)\n",
        "for group in file_groups:\n",
        "    preds, probs, labels, auc = evaluate_ensemble(group)\n",
        "    ensemble_preds_list.append(preds)\n",
        "    ensemble_probs_list.append(probs)\n",
        "    auc_scores.append(auc)\n",
        "\n",
        "# Combinar todos los ensembles en uno final\n",
        "majority_vote_with_auc(ensemble_preds_list, ensemble_probs_list, auc_scores, labels)"
      ],
      "metadata": {
        "id": "Wcn8GKUEEOmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
        "from scipy.special import expit\n",
        "from pathlib import Path\n",
        "\n",
        "# Lista de todos los modelos\n",
        "file_paths = [\n",
        "    \"./Rb/Bt1-1_preds\", \"./Rb/Bt1-2_preds\", \"./Rb/Bt1-3_preds\", \"./Rb/Bt1-4_preds\",\n",
        "    \"./Rb/Bt2-1_preds\", \"./Rb/Bt2-2_preds\", \"./Rb/Bt2-3_preds\", \"./Rb/Bt2-4_preds\",\n",
        "    \"./Rb/Bt3-1_preds\", \"./Rb/Bt3-2_preds\", \"./Rb/Bt3-3_preds\", \"./Rb/Bt3-4_preds\",\n",
        "    \"./Rb/Bt4-1_preds\", \"./Rb/Bt4-2_preds\", \"./Rb/Bt4-3_preds\", \"./Rb/Bt4-4_preds\",\n",
        "    \"./Rb/Bt5-1_preds\", \"./Rb/Bt5-2_preds\", \"./Rb/Bt5-3_preds\", \"./Rb/Bt5-4_preds\"\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "for path in file_paths:\n",
        "    filepath = Path(path)\n",
        "    df = pd.read_csv(filepath)\n",
        "    labels = df['label'].values\n",
        "    probs = expit(df['probability_class_1'].values)\n",
        "    preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    prec = precision_score(labels, preds, average='macro')\n",
        "    rec = recall_score(labels, preds, average='macro')\n",
        "    f1_macro = f1_score(labels, preds, average='macro')\n",
        "    f1_pos = f1_score(labels, preds, pos_label=1)\n",
        "    f1_neg = f1_score(labels, preds, pos_label=0)\n",
        "    auc = roc_auc_score(labels, probs)\n",
        "    avg_prec = average_precision_score(labels, probs)\n",
        "\n",
        "    results.append({\n",
        "        \"Modelo\": filepath.name,\n",
        "        \"Accuracy\": acc,\n",
        "        \"Precision (macro)\": prec,\n",
        "        \"Recall (macro)\": rec,\n",
        "        \"F1 (macro)\": f1_macro,\n",
        "        \"F1 clase 1\": f1_pos,\n",
        "        \"F1 clase 0\": f1_neg,\n",
        "        \"ROC AUC\": auc,\n",
        "        \"Average Precision\": avg_prec\n",
        "    })\n",
        "\n",
        "# Crear DataFrame con resultados\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df_sorted = results_df.sort_values(by=\"Modelo\").reset_index(drop=True)\n",
        "results_df_sorted\n"
      ],
      "metadata": {
        "id": "ndtW6zpLETv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
        "from scipy.special import expit\n",
        "from pathlib import Path\n",
        "\n",
        "# Lista de todos los modelos\n",
        "file_paths = [\n",
        "    \"./Rb/Bt1-1_preds\", \"./Rb/Bt1-2_preds\", \"./Rb/Bt1-3_preds\", \"./Rb/Bt1-4_preds\",\n",
        "    \"./Rb/Bt2-1_preds\", \"./Rb/Bt2-2_preds\", \"./Rb/Bt2-3_preds\", \"./Rb/Bt2-4_preds\",\n",
        "    \"./Rb/Bt3-1_preds\", \"./Rb/Bt3-2_preds\", \"./Rb/Bt3-3_preds\", \"./Rb/Bt3-4_preds\",\n",
        "    \"./Rb/Bt4-1_preds\", \"./Rb/Bt4-2_preds\", \"./Rb/Bt4-3_preds\", \"./Rb/Bt4-4_preds\",\n",
        "    \"./Rb/Bt5-1_preds\", \"./Rb/Bt5-2_preds\", \"./Rb/Bt5-3_preds\", \"./Rb/Bt5-4_preds\"\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "for path in file_paths:\n",
        "    filepath = Path(path)\n",
        "    df = pd.read_csv(filepath)\n",
        "    labels = df['label'].values\n",
        "    probs = expit(df['probability_class_1'].values)\n",
        "    preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    prec = precision_score(labels, preds, average='macro')\n",
        "    rec = recall_score(labels, preds, average='macro')\n",
        "    f1_macro = f1_score(labels, preds, average='macro')\n",
        "    f1_pos = f1_score(labels, preds, pos_label=1)\n",
        "    f1_neg = f1_score(labels, preds, pos_label=0)\n",
        "    auc = roc_auc_score(labels, probs)\n",
        "    avg_prec = average_precision_score(labels, probs)\n",
        "\n",
        "    results.append({\n",
        "        \"Modelo\": filepath.name,\n",
        "        \"Accuracy\": acc,\n",
        "        \"Precision (macro)\": prec,\n",
        "        \"Recall (macro)\": rec,\n",
        "        \"F1 (macro)\": f1_macro,\n",
        "        \"F1 clase 1\": f1_pos,\n",
        "        \"F1 clase 0\": f1_neg,\n",
        "        \"ROC AUC\": auc,\n",
        "        \"Average Precision\": avg_prec\n",
        "    })\n",
        "\n",
        "# Crear DataFrame con resultados\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df_sorted = results_df.sort_values(by=\"Modelo\").reset_index(drop=True)\n",
        "results_df_sorted"
      ],
      "metadata": {
        "id": "LIB5dfzqEZzL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Bpb7FOpsDw2a"
      ]
    },
    "kernelspec": {
      "display_name": "cudaenv",
      "language": "python",
      "name": "cudaenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}